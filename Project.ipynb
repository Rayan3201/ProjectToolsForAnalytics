{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1uAUJGEUzfNj6OsWNAimnYCw7eKaHhMUfU1MTj9YwYw4/edit?usp=sharing), [grading rubric](https://docs.google.com/document/d/1hKuRWqFcIdhOkow3Nljcm7PXzIkoa9c_aHkMKZDxWa0/edit?usp=sharing)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only an outline to help you with your own approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdb5f81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0532e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03d6135d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2884aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d8557a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (1.4.32)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from sqlalchemy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d347341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (0.12.2)\n",
      "Requirement already satisfied: pyproj>=2.6.1.post1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (3.5.0)\n",
      "Requirement already satisfied: packaging in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: shapely>=1.7 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (2.0.1)\n",
      "Requirement already satisfied: fiona>=1.8 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (1.9.3)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from geopandas) (2.0.0)\n",
      "Requirement already satisfied: munch>=2.3.2 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (21.4.0)\n",
      "Requirement already satisfied: click~=8.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (8.0.4)\n",
      "Requirement already satisfied: certifi in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (2021.10.8)\n",
      "Requirement already satisfied: cligj>=0.5 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from fiona>=1.8->geopandas) (4.11.3)\n",
      "Requirement already satisfied: six in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata->fiona>=1.8->geopandas) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from packaging->geopandas) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b92d8ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from requests) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import math\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"data/taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_CSV = \"\"\n",
    "WEATHER_CSV_DIR = \"\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a9a9933",
   "metadata": {},
   "source": [
    "Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d421c550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def load_taxi_zones(shapefile):\n",
    "   # if not os.path.isfile(shapefile):\n",
    "    #    raise ValueError(f\"Shapefile {shapefile} does not exist.\")\n",
    "   # gdf = gpd.read_file(shapefile)\n",
    "   # gdf.crs = f\"EPSG:{CRS}\"\n",
    "   # return gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "def load_taxi_zones():\n",
    "    shapefile_path = \"https://github.com/Rayan3201/ProjectToolsForAnalytics/blob/3194c900e0730dc0930d1498d0c476411c4a8bf4/taxi_zones.shp\"\n",
    "    taxi_zones = gpd.read_file(shapefile_path)\n",
    "    taxi_zones = taxi_zones[['LocationID', 'geometry']]\n",
    "    return taxi_zones\n",
    "\n",
    "def get_lat_lon(location_id, taxi_zones):\n",
    "    zone = taxi_zones[taxi_zones['LocationID'] == location_id]\n",
    "    if not zone.empty:\n",
    "        lat, lon = zone.geometry.centroid.y.values[0], zone.geometry.centroid.x.values[0]\n",
    "        return lat, lon\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f607acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones):\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "    if len(zone) == 0:\n",
    "        raise ValueError(f\"Zone with LocationID {zone_loc_id} not found.\")\n",
    "    lon, lat = zone.geometry.centroid.iloc[0].coords[0]\n",
    "    return lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79227ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import distance \n",
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    return distance(from_coord, to_coord).miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b336bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_zones(from_zone, to_zone):\n",
    "    from_lat, from_lon = lookup_coords_for_taxi_zone_id(from_zone, taxi_zones_dict)\n",
    "    to_lat, to_lon = lookup_coords_for_taxi_zone_id(to_zone, taxi_zones_dict)\n",
    "    if from_lat is None or to_lat is None:\n",
    "        return None\n",
    "    from_point = (from_lat, from_lon)\n",
    "    to_point = (to_lat, to_lon)\n",
    "    distance = geopy.distance.distance(from_point, to_point).km\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    R = 6373.0\n",
    "\n",
    "    lat1 = radians(from_coord[1])\n",
    "    lon1 = radians(from_coord[0])\n",
    "    lat2 = radians(to_coord[1])\n",
    "    lon2 = radians(to_coord[0])\n",
    "\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\"\"\" \n",
    "def add_distance_column(dataframe):\n",
    "    dataframe['distance'] = dataframe.apply(\n",
    "        lambda x: calculate_distance([x['pickup_latitude'],x['pickup_longitude']],\n",
    "                                     [x['dropoff_latitude'],x['dropoff_longitude']]),\n",
    "        axis =1\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "def add_distance_column(dataframe, taxi_zones):\n",
    "    dataframe['pickup_lat_lon'] = dataframe['PULocationID'].apply(lambda x: get_lat_lon(x, taxi_zones))\n",
    "    dataframe['dropoff_lat_lon'] = dataframe['DOLocationID'].apply(lambda x: get_lat_lon(x, taxi_zones))\n",
    "    \n",
    "    dataframe['distance'] = dataframe.apply(\n",
    "        lambda x: calculate_distance(x['pickup_lat_lon'], x['dropoff_lat_lon']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "First Half "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c12d7cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_all_urls_from_taxi_page(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Filter out the links that are not for parquet files\n",
    "    parquet_links = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and href.endswith('.parquet'):\n",
    "            parquet_links.append(href)\n",
    "\n",
    "    # Return the list of parquet file URLs\n",
    "    return parquet_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21dfcacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "parquet_links = get_all_urls_from_taxi_page(url)\n",
    "\n",
    "# Print the list of parquet file URLs\n",
    "print(parquet_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be33d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(parquet_links):\n",
    "    # Filter out the parquet file URLs for yellow taxi data only\n",
    "    yellow_taxi_links = []\n",
    "    for link in parquet_links:\n",
    "        if 'yellow_tripdata' in link:\n",
    "            yellow_taxi_links.append(link)\n",
    "    return yellow_taxi_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4d4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "parquet_links = get_all_urls_from_taxi_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba44aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_links = filter_taxi_parquet_urls(parquet_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_yellow_taxi_parquet_files(parquet_links, directory):\n",
    "    # Filter out the parquet file URLs for yellow taxi data only, from Jan 2009 to June 2015\n",
    "    yellow_taxi_links = []\n",
    "    for link in parquet_links:\n",
    "        if 'yellow_tripdata' in link:\n",
    "            match = re.search(r'yellow_tripdata_(\\d{4})-(\\d{2}).*', link)\n",
    "            year, month = int(match.group(1)), int(match.group(2))\n",
    "            if (year == 2009 and month >= 1) or (year >= 2010 and year <= 2014) or (year == 2015 and month <= 6):\n",
    "                yellow_taxi_links.append(link)\n",
    "\n",
    "    # Download yellow taxi parquet files to the specified directory\n",
    "    for url in yellow_taxi_links:\n",
    "        filename = url.split('/')[-1]\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(os.path.join(directory, filename), 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    \n",
    "    # Return the list of yellow taxi links\n",
    "    return yellow_taxi_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d812202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = '/Users/mikaelaballon/project_final/YELLOW PARQUET First half'\n",
    "\n",
    "# Call the function to obtain the list of yellow taxi links\n",
    "yellow_taxi_links = download_yellow_taxi_parquet_files(parquet_links, directory)\n",
    "\n",
    "# Loop over the filtered list of yellow taxi parquet file URLs and download each file\n",
    "for url in yellow_taxi_links:\n",
    "    download_yellow_taxi_parquet_files(url, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d32d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url):\n",
    "    # Define boundaries of New York City\n",
    "    nyc_box = (-74.242330, -73.717047, 40.560445, 40.908524)\n",
    "\n",
    "    # read the parquet file\n",
    "    dataframe = pd.read_parquet(url)\n",
    "\n",
    "    # Removing unnecessary columns and only keeping columns needed to answer questions in the other parts of this project\n",
    "    columns_to_keep = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'VendorID', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'fare_amount', 'tip_amount']\n",
    "    dataframe = dataframe[columns_to_keep]\n",
    "\n",
    "    #normalize the column name\n",
    "    dataframe.rename(columns = {\"tip_amount\":\"Tip_Amt\",\"tpep_pickup_datetime\":\"pickup_datetime\", 'tpep_dropoff_datetime':'dropoff_datetime'}, inplace = True)\n",
    "\n",
    "    # Remove values that are missing or invalid coordinates\n",
    "    lat_long_cols = ['PULocationID', 'DOLocationID']\n",
    "    for col in lat_long_cols:\n",
    "        if f'{col}_latitude' in dataframe.columns and f'{col}_longitude' in dataframe.columns:\n",
    "            invalid_coords = ((dataframe[f'{col}_latitude'] == 0) & (dataframe[f'{col}_longitude'] == 0))\n",
    "            dataframe = dataframe[~invalid_coords]\n",
    "\n",
    "    # Check if lat/long columns exist before attempting to extract them\n",
    "    if all([f'{col}_longitude' in dataframe.columns for col in lat_long_cols]):\n",
    "        # Remove trips outside the NYC boundaries\n",
    "        pickup_longitude = dataframe[f'{lat_long_cols[0]}_longitude']\n",
    "        pickup_latitude = dataframe[f'{lat_long_cols[0]}_latitude']\n",
    "        dropoff_longitude = dataframe[f'{lat_long_cols[1]}_longitude']\n",
    "        dropoff_latitude = dataframe[f'{lat_long_cols[1]}_latitude']\n",
    "        outside_nyc = (\n",
    "            (pickup_longitude < nyc_box[0]) |\n",
    "            (pickup_longitude > nyc_box[1]) |\n",
    "            (pickup_latitude < nyc_box[2]) |\n",
    "            (pickup_latitude > nyc_box[3]) |\n",
    "            (dropoff_longitude < nyc_box[0]) |\n",
    "            (dropoff_longitude > nyc_box[1]) |\n",
    "            (dropoff_latitude < nyc_box[2]) |\n",
    "            (dropoff_latitude > nyc_box[3])\n",
    "        )\n",
    "        dataframe = dataframe[~outside_nyc]\n",
    "\n",
    "    # Extracting date and hour from pickup_datetime column\n",
    "    dataframe['pickup_date'] = dataframe['pickup_datetime'].dt.date\n",
    "    dataframe['pickup_hour'] = dataframe['pickup_datetime'].dt.hour\n",
    "\n",
    "    # New column for trip duration in minutes\n",
    "    dataframe['trip_duration_minutes'] = (dataframe['dropoff_datetime'] - dataframe['pickup_datetime']).dt.seconds/60\n",
    "\n",
    "    # Dropping the original pickup_datetime and dropoff_datetime columns\n",
    "    dataframe.drop(['pickup_datetime', 'dropoff_datetime'], axis=1, inplace=True)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916994bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        add_distance_column(dataframe)\n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.contact(all_taxi_dataframes)\n",
    "    return taxi_data\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    taxi_zones = load_taxi_zones()  # Load taxi zones\n",
    "    taxi_data = pd.DataFrame()\n",
    "    for parquet_url in parquet_urls:\n",
    "        dataframe = get_and_clean_month(parquet_url)\n",
    "        add_distance_column(dataframe, taxi_zones)  # Pass taxi_zones here\n",
    "        taxi_data = pd.concat([taxi_data, dataframe], ignore_index=True)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls, dir_path)\n",
    "    taxi_data = get_and_clean_taxi_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "876bd645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2023-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2022-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2021-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2020-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2019-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2018-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2017-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2016-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2015-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2014-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2013-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2012-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2011-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2010-12.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-01.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-02.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-03.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-04.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-05.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-06.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-07.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-08.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-09.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-10.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-11.parquet has been downloaded.\n",
      "/Users/mikaelaballon/project_final/yello_taxis/yellow_tripdata_2009-12.parquet has been downloaded.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8b in position 19: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m taxi_data \u001b[39m=\u001b[39m get_taxi_data()\n",
      "\u001b[1;32m/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb Cell 32\u001b[0m in \u001b[0;36mget_taxi_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m all_urls \u001b[39m=\u001b[39m get_all_urls_from_taxi_page(TAXI_URL)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m all_parquet_urls \u001b[39m=\u001b[39m filter_taxi_parquet_urls(all_urls, dir_path)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m taxi_data \u001b[39m=\u001b[39m get_and_clean_taxi_data(all_parquet_urls)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mreturn\u001b[39;00m taxi_data\n",
      "\u001b[1;32m/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb Cell 32\u001b[0m in \u001b[0;36mget_and_clean_taxi_data\u001b[0;34m(parquet_urls)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     urllib\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murlretrieve(parquet_url, filename)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# file already and saved it before trying again\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m dataframe \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_table(filename)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m cleaned_dataframe \u001b[39m=\u001b[39m clean_taxi_data(dataframe)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m  \u001b[39m# maybe: if the file hasn't been saved, save it so you can\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mikaelaballon/ProjectToolsForAnalytics/Project.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# avoid re-downloading it if you re-run the function\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1242\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1229\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1230\u001b[0m     dialect,\n\u001b[1;32m   1231\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1239\u001b[0m )\n\u001b[1;32m   1240\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1242\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1678\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1679\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1680\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mif\u001b[39;00m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype_backend\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[39m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[39m\"\u001b[39m\u001b[39mpyarrow\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:548\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:637\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:2017\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8b in position 19: invalid start byte"
     ]
    }
   ],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc75f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_and_clean_uber_data(csv_file):\n",
    "    dataframe = pd.read_csv(r'C:/Users/mikaelaballon/project_final/uber/uber_rides_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ecaa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "UBER_DATA = \"/Users/mikaelaballon/project_final/uber/uber_rides_sample.csv\"\n",
    "\n",
    "def load_and_clean_uber_data(UBER_DATA):\n",
    "    dataframe = pd.read_csv(UBER_DATA)\n",
    "\n",
    "    dataframe = dataframe.drop(['Unnamed: 0','key'], axis=1)\n",
    "    dataframe = dataframe.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude','dropoff_longitude'])\n",
    "\n",
    "    min_lat, min_lon = 40.560445, -74.242330\n",
    "    max_lat, max_lon = 40.908524, -73.717047\n",
    "\n",
    "    dataframe = dataframe[(dataframe['pickup_latitude'].between(min_lat, max_lat)) &\n",
    "                          (dataframe['pickup_longitude'].between(min_lon, max_lon)) &\n",
    "                          (dataframe['dropoff_latitude'].between(min_lat, max_lat)) &\n",
    "                          (dataframe['dropoff_longitude'].between(min_lon, max_lon))]\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78ad5702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopy in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (2.3.0)\n",
      "Requirement already satisfied: geographiclib<3,>=1.52 in /Users/mikaelaballon/opt/anaconda3/lib/python3.9/site-packages (from geopy) (2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78aa9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b8fc4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import geodesic\n",
    "\n",
    "def add_distance_column(uber_dataframe):\n",
    "    pickup_coords = list(zip(uber_dataframe['pickup_latitude'], uber_dataframe['pickup_longitude']))\n",
    "    dropoff_coords = list(zip(uber_dataframe['dropoff_latitude'], uber_dataframe['dropoff_longitude']))\n",
    "    distances = [geodesic(pickup, dropoff).km for pickup, dropoff in zip(pickup_coords, dropoff_coords)]\n",
    "    uber_dataframe['distance_km'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e9c113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "directory = '/Users/mikaelaballon/project_final/weather data'\n",
    "\n",
    "def get_all_weather_csvs(directory):\n",
    "    csv_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_files.append(os.path.join(directory, filename))\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    # Load the CSV file into a Pandas dataframe\n",
    "    dataframe = pd.read_csv(csv_file)\n",
    "\n",
    "    # Convert 'date_time' column to datetime data type\n",
    "    dataframe['date_time'] = pd.to_datetime(dataframe['date_time'])\n",
    "\n",
    "    # Set 'date_time' column as the index\n",
    "    dataframe.set_index('date_time', inplace=True)\n",
    "\n",
    "    # Resample the data to hourly frequency and interpolate missing values\n",
    "    dataframe = dataframe.resample('H').mean().interpolate()\n",
    "\n",
    "    # Removing unnecessary columns and only keeping columns needed to answer questions in the other parts of this project\n",
    "    columns_to_keep = ['pickup_datetime', 'dropoff_datetime', 'pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude', 'fare_amount', 'passenger_count']\n",
    "    dataframe = dataframe.loc[:, columns_to_keep]\n",
    "\n",
    "    # Remove values that are missing or invalid coordinates\n",
    "    dataframe.dropna(subset=['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude'], inplace=True)\n",
    "    invalid_coords = ((dataframe['pickup_latitude'] == 0) & (dataframe['pickup_longitude'] == 0) & (dataframe['dropoff_latitude'] == 0) & (dataframe['dropoff_longitude'] == 0))\n",
    "    dataframe = dataframe[~invalid_coords]\n",
    "\n",
    "    # Extracting date and hour from pickup_datetime column\n",
    "    dataframe['pickup_date'] = dataframe.index.date\n",
    "    dataframe['pickup_hour'] = dataframe.index.hour\n",
    "\n",
    "    # New column for trip duration in minutes\n",
    "    dataframe['trip_duration_minutes'] = (dataframe['dropoff_datetime'] - dataframe['pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "    # Ensure all trips have positive times and are not over two hours\n",
    "    dataframe = dataframe[(dataframe['trip_duration_minutes'] > 0) & (dataframe['trip_duration_minutes'] < 120)]\n",
    "\n",
    "    # Remove trips with no fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] > 0]\n",
    "\n",
    "    # Define boundaries of New York City\n",
    "    nyc_box = (-74.242330, -73.717047, 40.560445, 40.908524)\n",
    "\n",
    "    # Remove trips that are outside the New York boundaries\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] >= nyc_box[0]) & (dataframe['pickup_longitude'] <= nyc_box[1]) &\n",
    "    (dataframe['pickup_latitude'] >= nyc_box[2]) & (dataframe['pickup_latitude'] <= nyc_box[3]) &\n",
    "    (dataframe['dropoff_longitude'] >= nyc_box[0]) & (dataframe['dropoff_longitude'] <= nyc_box[1]) &\n",
    "    (dataframe['dropoff_latitude'] >= nyc_box[2]) & (dataframe['dropoff_latitude'] <= nyc_box[3])]\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    # Load the CSV file into a pandas dataframe\n",
    "    dataframe = pd.read_csv(csv_file)\n",
    "\n",
    "    # Removing unnecessary columns and only keeping columns needed to answer questions in the other parts of this project\n",
    "    columns_to_keep = ['pickup_datetime', 'fare_amount', 'passenger_count']\n",
    "    dataframe = dataframe.loc[:, columns_to_keep]\n",
    "\n",
    "    # Converting datetime column to datetime data type\n",
    "    dataframe['pickup_datetime'] = pd.to_datetime(dataframe['pickup_datetime'])\n",
    "\n",
    "    # Extracting date from pickup_datetime column\n",
    "    dataframe['pickup_date'] = dataframe['pickup_datetime'].dt.date\n",
    "\n",
    "    # Dropping the original pickup_datetime column\n",
    "    dataframe.drop(['pickup_datetime'], axis=1, inplace=True)\n",
    "\n",
    "    # Remove trips with no fare\n",
    "    dataframe = dataframe[dataframe['fare_amount'] > 0]\n",
    "\n",
    "    # Define boundaries of New York City\n",
    "    nyc_box = (-74.242330, -73.717047, 40.560445, 40.908524)\n",
    "\n",
    "    # Remove trips that are outside the New York boundaries\n",
    "    dataframe = dataframe[(dataframe['pickup_longitude'] >= nyc_box[0]) & (dataframe['pickup_longitude'] <= nyc_box[1]) &\n",
    "    (dataframe['pickup_latitude'] >= nyc_box[2]) & (dataframe['pickup_latitude'] <= nyc_box[3])]\n",
    "\n",
    "    # Group data by date and calculate the total fare and passenger count for each date\n",
    "    daily_data = dataframe.groupby(['pickup_date']).agg({'fare_amount': 'sum', 'passenger_count': 'sum'}).reset_index()\n",
    "\n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae77681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(directory)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m engine \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39mcreate_engine(DATABASE_URL)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    datetime TEXT,\n",
    "    temperature REAL,\n",
    "    precipitation REAL,\n",
    "    wind_speed REAL,\n",
    "    visibility REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date TEXT,\n",
    "    max_temperature REAL,\n",
    "    min_temperature REAL,\n",
    "    precipitation REAL,\n",
    "    wind_speed REAL,\n",
    "    visibility REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime TEXT,\n",
    "    dropoff_datetime TEXT,\n",
    "    passenger_count INTEGER,\n",
    "    trip_distance REAL,\n",
    "    pickup_latitude REAL,\n",
    "    pickup_longitude REAL,\n",
    "    dropoff_latitude REAL,\n",
    "    dropoff_longitude REAL,\n",
    "    distance REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime TEXT,\n",
    "    dropoff_datetime TEXT,\n",
    "    passenger_count INTEGER,\n",
    "    trip_distance REAL,\n",
    "    pickup_latitude REAL,\n",
    "    pickup_longitude REAL,\n",
    "    dropoff_latitude REAL,\n",
    "    dropoff_longitude REAL,\n",
    "    distance REAL\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    connection.execute(HOURLY_WEATHER_SCHEMA)\n",
    "    connection.execute(DAILY_WEATHER_SCHEMA)\n",
    "    connection.execute(TAXI_TRIPS_SCHEMA)\n",
    "    connection.execute(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    for table_name, df in table_to_df_dict.items():\n",
    "        df.to_sql(table_name, engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "QUERY_1_FILENAME = \"q1.sq1\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "WITH hourly_pickups AS (\n",
    "    SELECT strftime('%H', pickup_datetime) AS hour, COUNT(*) AS num_rides\n",
    "    FROM taxi_trips\n",
    "    WHERE strftime('%Y-%m', pickup_datetime) BETWEEN '2009-01' AND '2015-06'\n",
    "    GROUP BY hour\n",
    ")\n",
    "SELECT hour, num_rides\n",
    "FROM hourly_pickups\n",
    "ORDER BY num_rides DESC;\n",
    "\"\"\"\n",
    "\n",
    "QUERY_2_FILENAME = \"q2.sq1\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "WITH daily_pickups AS (\n",
    "    SELECT strftime('%w', pickup_datetime) AS day_of_week, COUNT(*) AS num_rides\n",
    "    FROM uber_trips\n",
    "    WHERE strftime('%Y-%m', pickup_datetime) BETWEEN '2009-01' AND '2015-06'\n",
    "    GROUP BY day_of_week\n",
    ")\n",
    "SELECT day_of_week, num_rides\n",
    "FROM daily_pickups\n",
    "ORDER BY num_rides DESC;\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "QUERY_3_FILENAME = \"q3.sq1\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH july_trips AS (\n",
    "    SELECT distance\n",
    "    FROM taxi_trips\n",
    "    WHERE strftime('%Y-%m', pickup_datetime) = '2013-07'\n",
    ")\n",
    "SELECT percentile_cont(0.95) WITHIN GROUP (ORDER BY distance) AS percentile_95\n",
    "FROM july_trips;\n",
    "\"\"\"\n",
    "\n",
    "QUERY_4_FILENAME = \"q4.sq1\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH daily_summary AS (\n",
    "    SELECT strftime('%Y-%m-%d', pickup_datetime) AS date,\n",
    "           COUNT(*) AS num_rides,\n",
    "           AVG(distance) AS avg_distance\n",
    "    FROM taxi_trips\n",
    "    WHERE strftime('%Y', pickup_datetime) = '2009'\n",
    "    GROUP BY date\n",
    ")\n",
    "SELECT date, num_rides, avg_distance\n",
    "FROM daily_summary\n",
    "ORDER BY num_rides DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "QUERY_5_FILENAME = \"q5.sq1\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH windiest_days AS (\n",
    "    SELECT date, AVG(wind_speed) AS avg_wind_speed\n",
    "    FROM daily_weather\n",
    "    WHERE strftime('%Y', date) = '2014'\n",
    "    GROUP BY date\n",
    "    ORDER BY avg_wind_speed DESC\n",
    "    LIMIT 10\n",
    "),\n",
    "trips_on_windiest_days AS (\n",
    "    SELECT windiest_days.date,\n",
    "           windiest_days.avg_wind_speed,\n",
    "           COUNT(taxi_trips.id) AS num_trips\n",
    "    FROM windiest_days\n",
    "    LEFT JOIN taxi_trips ON strftime('%Y-%m-%d', taxi_trips.pickup_datetime) = windiest_days.date\n",
    "    GROUP BY windiest_days.date, windiest_days.avg_wind_speed\n",
    ")\n",
    "SELECT date, avg_wind_speed, num_trips\n",
    "FROM trips_on_windiest_days\n",
    "ORDER BY avg\n",
    "\"\"\"\n",
    "\n",
    "QUERY_6_FILENAME = \"q6.sq1\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "engine.execute(QUERY_1).fetchall()\n",
    "engine.execute(QUERY_2).fetchall()\n",
    "engine.execute(QUERY_3).fetchall()\n",
    "engine.execute(QUERY_4).fetchall()\n",
    "engine.execute(QUERY_5).fetchall()\n",
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)\n",
    "write_query_to_file(QUERY_2, QUERY_1_FILENAME)\n",
    "write_query_to_file(QUERY_3, QUERY_1_FILENAME)\n",
    "write_query_to_file(QUERY_4, QUERY_1_FILENAME)\n",
    "write_query_to_file(QUERY_5, QUERY_1_FILENAME)\n",
    "write_query_to_file(QUERY_6, QUERY_1_FILENAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot_visual_1(dataframe):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='borough', y='average_fare', data=dataframe)\n",
    "    plt.title('Average Fare per Borough')\n",
    "    plt.xlabel('Borough')\n",
    "    plt.ylabel('Average Fare')\n",
    "    plt.show()\n",
    "\n",
    "def get_data_for_visual_1():\n",
    "    conn = sqlite3.connect(DATABASE_URL)\n",
    "    query = '''SELECT ... FROM ...'''\n",
    "    dataframe = pd.read_sql_query(QUERY_1, conn)\n",
    "    conn.close()\n",
    "    return dataframe\n",
    "\n",
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def plot_visual_2(dataframe):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='month', y='average_distance', data=dataframe)\n",
    "    plt.title('Average Distance Traveled per Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Distance')\n",
    "    plt.show()\n",
    "\n",
    "def get_data_for_visual_2():\n",
    "    conn = sqlite3.connect(DATABASE_URL)\n",
    "    query = '''\n",
    "        SELECT strftime('%m', date_column) AS month, AVG(distance_column) AS average_distance\n",
    "        FROM uber_trips, taxi_trips\n",
    "        GROUP BY month\n",
    "    '''\n",
    "    dataframe = pd.read_sql_query(QUERY_2, conn)\n",
    "    conn.close()\n",
    "    return dataframe\n",
    "\n",
    "some_dataframe = get_data_for_visual_2()\n",
    "plot_visual_2(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a770a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_3(dataframe):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='airport', y='dropoff_count', hue='day_of_week', data=dataframe)\n",
    "    plt.title('Drop-offs at Airports by Day of the Week')\n",
    "    plt.xlabel('Airport')\n",
    "    plt.ylabel('Drop-off Count')\n",
    "    plt.show()\n",
    "\n",
    "def get_data_for_visual_3():\n",
    "    conn = sqlite3.connect(DATABASE_URL)\n",
    "    query = '''\n",
    "        SELECT airport, strftime('%w', date_column) AS day_of_week, COUNT(*) AS dropoff_count\n",
    "        FROM uber_trips, taxi_trips\n",
    "        WHERE dropoff_location IN ('LGA', 'JFK', 'EWR')\n",
    "        GROUP BY airport, day_of_week\n",
    "    '''\n",
    "    dataframe = pd.read_sql_query(QUERY_3, conn)\n",
    "    conn.close()\n",
    "    return dataframe\n",
    "\n",
    "some_dataframe = get_data_for_visual_3()\n",
    "plot_visual_3(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6e9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "def plot_visual_4(dataframe):\n",
    "    m = folium.Map([40.7128, -74.0060], zoom_start=11)\n",
    "    \n",
    "    heatmap_data = dataframe[['latitude', 'longitude']].values.tolist()\n",
    "    \n",
    "    HeatMap(heatmap_data).add_to(m)\n",
    "    \n",
    "    return m\n",
    "\n",
    "def get_data_for_visual_4():\n",
    "    conn = sqlite3.connect(DATABASE_URL)\n",
    "    query = '''\n",
    "        SELECT latitude, longitude\n",
    "        FROM uber_trips, taxi_trips\n",
    "    '''\n",
    "    dataframe = pd.read_sql_query(QUERY_4, conn)\n",
    "    conn.close()\n",
    "    return dataframe\n",
    "\n",
    "some_dataframe = get_data_for_visual_4()\n",
    "plot_visual_4(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_5(dataframe):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='distance', y='tip_amount', data=dataframe)\n",
    "    plt.title('Tip Amount vs Distance for Yellow Taxi Rides')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Tip Amount')\n",
    "    plt.show()\n",
    "\n",
    "def get_data_for_visual_5():\n",
    "    conn = sqlite3.connect(DATABASE_URL)\n",
    "    query = '''\n",
    "        SELECT distance, tip_amount\n",
    "        FROM taxi_trips\n",
    "    '''\n",
    "    dataframe = pd.read_sql_query(QUERY_5, conn)\n",
    "    conn.close()\n",
    "    return dataframe\n",
    "\n",
    "some_dataframe = get_data_for_visual_5()\n",
    "plot_visual_5(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24baeb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_6(dataframe):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x='precipitation_amount', y='tip_amount', data=dataframe)\n",
    "    plt.title('Tip Amount vs Precipitation Amount for Yellow Taxi Rides')\n",
    "    plt.xlabel('Precipitation Amount (inches)')\n",
    "    plt.ylabel('Tip Amount ($)')\n",
    "    plt.show()\n",
    "\n",
    "def get_data_for_visual_6():\n",
    "    query = '''\n",
    "    SELECT\n",
    "        yellow_taxi.tip_amount,\n",
    "        weather.precipitation\n",
    "    FROM taxi_trips\n",
    "    JOIN weather ON yellow_taxi.pickup_datetime = weather.date_time\n",
    "    WHERE yellow_taxi.tip_amount > 0\n",
    "    '''\n",
    "    dataframe = pd.read_sql(QUERY_6, conn)\n",
    "    conn.close()\n",
    "    return dataframe\n",
    "\n",
    "some_dataframe = get_data_for_visual_5()\n",
    "plot_visual_5(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.11.0 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0cd191",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
